# Continuous Learning System - User Guide

## ğŸš€ Quick Start

### 1. Initialize the System (One-Time Setup)
```bash
python src/initialize_cl_system.py
```

This will:
- Copy your current model to the active directory
- Create initial metadata and versioning
- Set up the master training dataset
- Initialize deployment logs

### 2. Use the Dashboard
```bash
streamlit run src/app.py
```

- Upload new customer data via the sidebar
- Data is automatically saved for future model training
- Dashboard always uses the latest deployed model

### 3. Manual Retraining (Optional)
```bash
python src/retrain_manual.py
```

Manually trigger the retraining pipeline when you have new data.

### 4. Start Automated Scheduler (Recommended)
```bash
python src/ml_pipeline/scheduler.py
```

Runs daily at 2:00 AM to check for new data and retrain if needed.

---

## ğŸ“‹ System Workflow

```
User Uploads CSV â†’ Saved to data/new/
                â†“
        (Daily at 2 AM)
                â†“
    Merge with Master Dataset
                â†“
        Train New Model
                â†“
    Validate (Accuracy, Drift)
                â†“
        Deploy if Approved
                â†“
    Dashboard Auto-Updates
```

---

## ğŸ› ï¸ Commands Reference

### View Deployment History
```bash
python src/rollback.py --list
```

### Rollback to Previous Version
```bash
python src/rollback.py
```

### Rollback to Specific Version
```bash
python src/rollback.py --version 1.2
```

### Check System Status
```python
from ml_pipeline.data_manager import DataManager
dm = DataManager()
print(dm.get_data_stats())
```

---

## ğŸ“ Directory Structure

```
data/
â”œâ”€â”€ new/              # Uploaded files (staging)
â”œâ”€â”€ training/         # Master dataset
â””â”€â”€ archive/          # Processed files

models/
â”œâ”€â”€ active/           # Current production model
â”‚   â”œâ”€â”€ model.pkl
â”‚   â””â”€â”€ metadata.json
â”œâ”€â”€ versions/         # Historical models
â”‚   â”œâ”€â”€ model_v1_0.pkl
â”‚   â”œâ”€â”€ model_v1_1.pkl
â”‚   â””â”€â”€ ...
â””â”€â”€ metadata/         # Model performance logs
    â”œâ”€â”€ model_v1_0.json
    â””â”€â”€ ...

logs/
â”œâ”€â”€ retraining.log    # Training logs
â””â”€â”€ deployments.json  # Deployment history
```

---

## âš™ï¸ Configuration

### Change Retraining Schedule
Edit `src/ml_pipeline/scheduler.py`:
```python
# Daily at 3 AM
scheduler.schedule_daily("03:00")

# Weekly on Monday at 2 AM
scheduler.schedule_weekly("monday", "02:00")
```

### Adjust Validation Thresholds
Edit `src/ml_pipeline/model_validator.py`:
```python
self.min_accuracy = 0.70        # Minimum accuracy required
self.max_accuracy_drop = 0.05   # Max 5% drop allowed
self.max_drift_threshold = 0.15 # Max 15% drift
```

---

## ğŸ” Monitoring

### Check Logs
```bash
# View retraining logs
cat logs/retraining.log

# View deployment history
cat logs/deployments.json
```

### Model Metrics
All model versions include:
- Accuracy
- Precision
- Recall
- F1 Score
- AUC
- Feature Importance
- Confusion Matrix

---

## ğŸš¨ Troubleshooting

### Model Not Updating
1. Check if new files exist in `data/new/`
2. Review `logs/retraining.log` for errors
3. Verify scheduler is running

### Validation Failing
- Check validation thresholds in `model_validator.py`
- Review metrics comparison in logs
- Consider if new data quality is poor

### Rollback Needed
```bash
python src/rollback.py --list  # See history
python src/rollback.py         # Rollback to previous
```

---

## ğŸ“Š Dashboard Integration

The dashboard automatically:
- âœ… Uses the latest deployed model
- âœ… Saves uploaded data for training
- âœ… Displays current model version
- âœ… Shows deployment date

No manual intervention needed!

---

## ğŸ” Safety Features

1. **Validation Before Deployment**
   - Accuracy must meet minimum threshold
   - Cannot drop more than 5% from current model
   - Drift detection prevents bad deployments

2. **Version Control**
   - All models are versioned and saved
   - Full metadata for each version
   - Easy rollback to any previous version

3. **Logging**
   - All operations logged
   - Deployment history tracked
   - Audit trail maintained

---

## ğŸ’¡ Best Practices

1. **Monitor Regularly**: Check logs weekly
2. **Validate Data**: Ensure uploaded data quality
3. **Test Rollback**: Practice rollback procedure
4. **Keep Versions**: Don't delete old model versions
5. **Review Metrics**: Compare model performance over time

---

## ğŸ“ Support

For issues or questions:
1. Check `logs/retraining.log`
2. Review this guide
3. Contact ML team
